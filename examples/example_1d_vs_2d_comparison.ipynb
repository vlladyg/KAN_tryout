{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D vs 2D Lookup KAN Comparison\n",
    "\n",
    "This notebook compares 1D and 2D lookup-based Kolmogorov-Arnold Networks (KANs) for approximating a random MLP function as described in the paper.\n",
    "\n",
    "**Target Function**: Randomly-initialized MLP R^32 \u2192 R^1 with frozen weights\n",
    "\n",
    "**Key Questions**:\n",
    "- How do 1D vs 2D lookup KANs compare in parameter efficiency?\n",
    "- What do the basis functions look like?\n",
    "- How well can each approximate the target function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, using CPU (will be slow)\")\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify lmKAN Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from lmKAN import LMKAN2DLayer\n",
    "    from lmKAN import utilities\n",
    "    print(\"\u2713 lmKAN imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u2717 lmKAN import failed: {e}\")\n",
    "    print(\"\\\\nPlease install lmKAN first:\")\n",
    "    print(\"  cd lmkan\")\n",
    "    print(\"  pip install .\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions: CDF Grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_grid_function(x):\n",
    "    \"\"\"Laplace CDF: maps R -> [0, 1]\"\"\"\n",
    "    absolute = np.abs(x)\n",
    "    value = 0.5 * np.exp(-absolute)\n",
    "    if x > 0:\n",
    "        return 1.0 - value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def inverse_grid_function(x):\n",
    "    \"\"\"Inverse Laplace CDF: maps [0, 1] -> R\"\"\"\n",
    "    if x <= 0.5:\n",
    "        return np.log(2.0 * x)\n",
    "    else:\n",
    "        return -np.log(2.0 * (1.0 - x))\n",
    "\n",
    "def get_borders_cdf_grid(n_chunks):\n",
    "    \"\"\"Get non-uniform grid borders using Laplace CDF\"\"\"\n",
    "    chunk_size = 1.0 / n_chunks\n",
    "    borders = []\n",
    "    for i in range(1, n_chunks):\n",
    "        level_now = i * chunk_size\n",
    "        borders.append(inverse_grid_function(level_now))\n",
    "    left_most = borders[0] - (borders[1] - borders[0])\n",
    "    right_most = borders[-1] + (borders[-1] - borders[-2])\n",
    "    return [left_most] + borders + [right_most]\n",
    "\n",
    "# Test the grid\n",
    "test_borders = get_borders_cdf_grid(8)\n",
    "print(f\"Grid with 8 chunks has {len(test_borders)} borders:\")\n",
    "print(f\"  {[f'{b:.3f}' for b in test_borders]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement 1D Lookup KAN Layer\n",
    "\n",
    "This is a simple PyTorch implementation of a 1D lookup KAN layer for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupKAN1D(nn.Module):\n",
    "    \"\"\"1D Lookup KAN Layer using linear interpolation on CDF grid\"\"\"\n",
    "    \n",
    "    def __init__(self, num_grids, input_dim, output_dim, init_scale=0.1):\n",
    "        super(LookupKAN1D, self).__init__()\n",
    "        self.num_grids = num_grids\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Parameters: [num_grids + 1, output_dim, input_dim]\n",
    "        # One 1D function per (input, output) pair\n",
    "        self.func_parameter = nn.Parameter(\n",
    "            torch.empty(num_grids + 1, output_dim, input_dim)\n",
    "        )\n",
    "        nn.init.uniform_(\n",
    "            self.func_parameter,\n",
    "            -init_scale / math.sqrt(input_dim),\n",
    "            init_scale / math.sqrt(input_dim)\n",
    "        )\n",
    "        \n",
    "        # Register CDF grid borders\n",
    "        self.register_buffer(\n",
    "            'borders',\n",
    "            torch.tensor(get_borders_cdf_grid(num_grids), dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Apply Laplace CDF to map to [0, 1]\n",
    "        func_value = torch.exp(-torch.abs(x))\n",
    "        x_cdf = torch.where(\n",
    "            x > 0,\n",
    "            1.0 - 0.5 * func_value,\n",
    "            0.5 * func_value\n",
    "        )\n",
    "        \n",
    "        # Map to grid indices\n",
    "        x_scaled = x_cdf * self.num_grids\n",
    "        indices = torch.clamp(x_scaled.long(), 0, self.num_grids - 1)\n",
    "        \n",
    "        # Compute interpolation weights\n",
    "        delta = x_scaled - indices.float()\n",
    "        delta = torch.clamp(delta, 0.0, 1.0)\n",
    "        \n",
    "        # Linear interpolation\n",
    "        # For each (batch, input, output): interpolate between two grid points\n",
    "        output = torch.zeros(batch_size, self.output_dim, device=x.device)\n",
    "        \n",
    "        for out_idx in range(self.output_dim):\n",
    "            for in_idx in range(self.input_dim):\n",
    "                grid_idx = indices[:, in_idx]  # [batch_size]\n",
    "                d = delta[:, in_idx]  # [batch_size]\n",
    "                \n",
    "                # Get values at grid points\n",
    "                val_left = self.func_parameter[grid_idx, out_idx, in_idx]\n",
    "                val_right = self.func_parameter[torch.clamp(grid_idx + 1, 0, self.num_grids), out_idx, in_idx]\n",
    "                \n",
    "                # Linear interpolation\n",
    "                interp_val = (1 - d) * val_left + d * val_right\n",
    "                output[:, out_idx] += interp_val\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_hessian_regularization(self):\n",
    "        \"\"\"Simple finite difference regularization for 1D functions\"\"\"\n",
    "        # Second derivative: f''(x) \u2248 (f[i+1] - 2*f[i] + f[i-1]) / h^2\n",
    "        second_deriv = self.func_parameter[2:] - 2*self.func_parameter[1:-1] + self.func_parameter[:-2]\n",
    "        return torch.mean(second_deriv ** 2)\n",
    "\n",
    "# Test 1D KAN\n",
    "kan1d = LookupKAN1D(num_grids=8, input_dim=32, output_dim=1).to(device)\n",
    "test_input = torch.randn(16, 32).to(device)\n",
    "test_output = kan1d(test_input)\n",
    "print(f\"1D KAN test:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in kan1d.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup 2D Lookup KAN Layer (lmKAN)\n",
    "\n",
    "Wrapper for the existing LMKAN2DLayer with batch normalization as recommended in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupKAN2DModel(nn.Module):\n    \"\"\"2D Lookup KAN model with BatchNorm\"\"\"\n    \n    def __init__(self, num_grids, input_dim, hidden_dim, output_dim):\n        super(LookupKAN2DModel, self).__init__()\n        # BatchNorm with affine=False as recommended in paper section 3.1\n        self.bn = nn.BatchNorm1d(input_dim, affine=False)\n        \n        # 2D LMKAN layer (hidden_dim must be divisible by tile_size=8)\n        self.kan = LMKAN2DLayer(\n            num_grids=num_grids,\n            input_dim=input_dim,\n            output_dim=hidden_dim,\n            tile_size_forward=8,\n            tile_size_backward=4,\n            block_size_forward=1024,\n            block_size_backward=512,\n        )\n        # Output layer to project to final dimension\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        # x: [batch_size, input_dim]\n        x = self.bn(x)  # Apply batch norm\n        x = x.T.contiguous()  # Convert to batch-last layout: [input_dim, batch_size]\n        x = self.kan(x)  # [hidden_dim, batch_size]\n        x = x.T  # Convert back: [batch_size, hidden_dim]\n        return self.output_layer(x)  # [batch_size, output_dim]\n    \n    def get_hessian_regularization(self):\n        return self.kan.get_hessian_regularization()\n\n# Test 2D KAN\nif torch.cuda.is_available():\n    kan2d = LookupKAN2DModel(num_grids=8, input_dim=32, hidden_dim=8, output_dim=1).cuda()\n    test_input = torch.randn(16, 32).cuda()\n    test_output = kan2d(test_input)\n    print(f\"2D KAN test:\")\n    print(f\"  Input shape: {test_input.shape}\")\n    print(f\"  Output shape: {test_output.shape}\")\n    print(f\"  Parameters: {sum(p.numel() for p in kan2d.parameters()):,}\")\nelse:\n    print(\"Skipping 2D KAN test (requires CUDA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Target Function: Random MLP\n",
    "\n",
    "As described in the paper: a randomly-initialized MLP R^32 \u2192 R^1 with frozen weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create target MLP\n",
    "target_mlp = nn.Sequential(\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ").to(device)\n",
    "\n",
    "# Freeze all parameters\n",
    "target_mlp.eval()\n",
    "for p in target_mlp.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"Target MLP:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in target_mlp.parameters()):,}\")\n",
    "print(f\"  Architecture: 32 \u2192 64 (ReLU) \u2192 1\")\n",
    "\n",
    "# Generate dataset\n",
    "def generate_dataset(n_samples, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    x = torch.randn(n_samples, 32).to(device)\n",
    "    with torch.no_grad():\n",
    "        y = target_mlp(x)\n",
    "    return x, y\n",
    "\n",
    "# Create train and test datasets\n",
    "train_x, train_y = generate_dataset(10000, seed=100)\n",
    "test_x, test_y = generate_dataset(2000, seed=200)\n",
    "\n",
    "print(f\"\\\\nDataset:\")\n",
    "print(f\"  Train: {train_x.shape[0]} samples\")\n",
    "print(f\"  Test: {test_x.shape[0]} samples\")\n",
    "print(f\"  Target output range: [{train_y.min():.3f}, {train_y.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Utilities\n",
    "\n",
    "Hessian regularization schedule as recommended in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitting_schedule(epoch_number):\n",
    "    \"\"\"Hessian regularization + learning rate schedule\"\"\"\n",
    "    HESSIAN_DECAY_EPOCHS = 100\n",
    "    HESSIAN_DECAY_SCALE = 15\n",
    "    COSINE_EPOCHS = 100\n",
    "    BASE_LR = 1e-3\n",
    "    INITIAL_HESSIAN_WEIGHT = 1.0\n",
    "\n",
    "    if epoch_number <= HESSIAN_DECAY_EPOCHS:\n",
    "        learning_rate = BASE_LR\n",
    "        hessian_regularization_lambda = INITIAL_HESSIAN_WEIGHT / (10 ** (epoch_number / HESSIAN_DECAY_SCALE))\n",
    "    else:\n",
    "        offset = epoch_number - HESSIAN_DECAY_EPOCHS\n",
    "        learning_rate = 0.5 * BASE_LR * (1.0 + math.cos(math.pi * offset / COSINE_EPOCHS))\n",
    "        hessian_regularization_lambda = 0.0\n",
    "\n",
    "    return learning_rate, hessian_regularization_lambda\n",
    "\n",
    "def train_model(model, train_x, train_y, test_x, test_y, num_epochs=200, batch_size=256, model_name=\"Model\"):\n",
    "    \"\"\"Train a KAN model with Hessian regularization\"\"\"\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    hessian_weights = []\n",
    "    \n",
    "    n_batches = (len(train_x) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        lr, hessian_lambda = get_fitting_schedule(epoch)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        perm = torch.randperm(len(train_x))\n",
    "        train_x_shuffled = train_x[perm]\n",
    "        train_y_shuffled = train_y[perm]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(train_x))\n",
    "            \n",
    "            batch_x = train_x_shuffled[start_idx:end_idx]\n",
    "            batch_y = train_y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_x)\n",
    "            \n",
    "            loss = F.mse_loss(pred, batch_y)\n",
    "            \n",
    "            if hessian_lambda > 0:\n",
    "                hessian_reg = model.get_hessian_regularization()\n",
    "                loss = loss + hessian_lambda * hessian_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= n_batches\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(test_x)\n",
    "            test_loss = F.mse_loss(test_pred, test_y).item()\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        hessian_weights.append(hessian_lambda)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{num_epochs}: Train Loss = {epoch_loss:.6f}, Test Loss = {test_loss:.6f}, LR = {lr:.6f}, \u03bb = {hessian_lambda:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses, hessian_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train and Compare Models\n",
    "\n",
    "Train both 1D and 2D KAN models on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\nNUM_GRIDS = 8\nNUM_EPOCHS = 200\n\nprint(\"=\"*60)\nprint(\"Model Comparison\")\nprint(\"=\"*60)\n\n# 1D KAN\nmodel_1d = LookupKAN1D(num_grids=NUM_GRIDS, input_dim=32, output_dim=1).to(device)\nparams_1d = sum(p.numel() for p in model_1d.parameters())\nprint(f\"\\n1D KAN: {params_1d:,} parameters\")\n\n# 2D KAN\nif torch.cuda.is_available():\n    model_2d = LookupKAN2DModel(num_grids=NUM_GRIDS, input_dim=32, hidden_dim=8, output_dim=1).cuda()\n    params_2d = sum(p.numel() for p in model_2d.parameters())\n    print(f\"2D KAN: {params_2d:,} parameters\")\n    print(f\"Parameter ratio (2D/1D): {params_2d/params_1d:.2f}x\")\nelse:\n    print(\"\\nWARNING: Skipping 2D KAN (requires CUDA)\")\n    model_2d = None\n\nprint(f\"\\nTarget MLP: {sum(p.numel() for p in target_mlp.parameters()):,} parameters\")\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1D KAN\n",
    "print(\"\\nTraining 1D KAN...\")\n",
    "start_time = time.time()\n",
    "train_losses_1d, test_losses_1d, hessian_1d = train_model(\n",
    "    model_1d, train_x, train_y, test_x, test_y, \n",
    "    num_epochs=NUM_EPOCHS, model_name=\"1D KAN\"\n",
    ")\n",
    "time_1d = time.time() - start_time\n",
    "print(f\"1D KAN training time: {time_1d:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 2D KAN\n",
    "if model_2d is not None:\n",
    "    print(\"\\nTraining 2D KAN...\")\n",
    "    start_time = time.time()\n",
    "    train_losses_2d, test_losses_2d, hessian_2d = train_model(\n",
    "        model_2d, train_x, train_y, test_x, test_y,\n",
    "        num_epochs=NUM_EPOCHS, model_name=\"2D KAN\"\n",
    "    )\n",
    "    time_2d = time.time() - start_time\n",
    "    print(f\"2D KAN training time: {time_2d:.2f}s\")\n",
    "else:\n",
    "    train_losses_2d = test_losses_2d = hessian_2d = None\n",
    "    time_2d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "axes[0].semilogy(train_losses_1d, label='1D KAN Train', linewidth=2)\n",
    "axes[0].semilogy(test_losses_1d, label='1D KAN Test', linewidth=2, linestyle='--')\n",
    "if train_losses_2d is not None:\n",
    "    axes[0].semilogy(train_losses_2d, label='2D KAN Train', linewidth=2)\n",
    "    axes[0].semilogy(test_losses_2d, label='2D KAN Test', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss (log scale)', fontsize=12)\n",
    "axes[0].set_title('Training Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Hessian regularization weight\n",
    "axes[1].semilogy(hessian_1d, label='Hessian Weight \u03bb', linewidth=2, color='red')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Regularization Weight (log scale)', fontsize=12)\n",
    "axes[1].set_title('Hessian Regularization Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1D KAN:\")\n",
    "print(f\"  Final Test MSE: {test_losses_1d[-1]:.6f}\")\n",
    "print(f\"  Training Time: {time_1d:.2f}s\")\n",
    "print(f\"  Parameters: {params_1d:,}\")\n",
    "\n",
    "if train_losses_2d is not None:\n",
    "    print(f\"\\n2D KAN:\")\n",
    "    print(f\"  Final Test MSE: {test_losses_2d[-1]:.6f}\")\n",
    "    print(f\"  Training Time: {time_2d:.2f}s\")\n",
    "    print(f\"  Parameters: {params_2d:,}\")\n",
    "    print(f\"\\nImprovement: {(test_losses_1d[-1] / test_losses_2d[-1]):.2f}x better MSE with 2D KAN\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize CDF Grid\n",
    "\n",
    "The non-uniform grid based on Laplace CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CDF grid\n",
    "borders = get_borders_cdf_grid(NUM_GRIDS)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Grid borders on real line\n",
    "axes[0].scatter(borders, np.zeros_like(borders), s=100, c='red', zorder=3)\n",
    "axes[0].vlines(borders, -0.1, 0.1, colors='red', alpha=0.5, linewidth=2)\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_title(f'CDF Grid Borders (N={NUM_GRIDS} chunks)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(-0.2, 0.2)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# CDF function\n",
    "x_vals = np.linspace(-5, 5, 1000)\n",
    "cdf_vals = np.array([direct_grid_function(x) for x in x_vals])\n",
    "axes[1].plot(x_vals, cdf_vals, linewidth=2, label='Laplace CDF')\n",
    "axes[1].scatter(borders, [direct_grid_function(b) for b in borders], \n",
    "                s=100, c='red', zorder=3, label='Grid Points')\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('CDF(x)', fontsize=12)\n",
    "axes[1].set_title('Laplace CDF Transform', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Grid statistics:\")\n",
    "print(f\"  Number of borders: {len(borders)}\")\n",
    "print(f\"  Range: [{min(borders):.3f}, {max(borders):.3f}]\")\n",
    "print(f\"  Central spacing: {borders[NUM_GRIDS//2+1] - borders[NUM_GRIDS//2]:.3f}\")\n",
    "print(f\"  Edge spacing: {borders[-1] - borders[-2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize 1D Spline Basis Functions\n",
    "\n",
    "Linear interpolation basis functions (\\\"hat\\\" functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_1d_basis_functions(x_vals, borders):\n",
    "    \"\"\"Compute all 1D linear basis functions on x_vals\"\"\"\n",
    "    n_grids = len(borders) - 1\n",
    "    basis_funcs = np.zeros((len(x_vals), n_grids + 1))\n",
    "    \n",
    "    for i, x in enumerate(x_vals):\n",
    "        grid_idx = np.searchsorted(borders, x) - 1\n",
    "        grid_idx = np.clip(grid_idx, 0, n_grids - 1)\n",
    "        \n",
    "        left_border = borders[grid_idx]\n",
    "        right_border = borders[grid_idx + 1]\n",
    "        delta = (x - left_border) / (right_border - left_border)\n",
    "        delta = np.clip(delta, 0.0, 1.0)\n",
    "        \n",
    "        # Linear basis functions (hat functions)\n",
    "        basis_funcs[i, grid_idx] = 1 - delta\n",
    "        basis_funcs[i, grid_idx + 1] = delta\n",
    "    \n",
    "    return basis_funcs\n",
    "\n",
    "# Compute and plot 1D basis functions\n",
    "x_range = np.linspace(borders[0], borders[-1], 1000)\n",
    "basis_1d = compute_1d_basis_functions(x_range, borders)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot all basis functions\n",
    "for i in range(len(borders)):\n",
    "    axes[0].plot(x_range, basis_1d[:, i], linewidth=2, alpha=0.7, label=f'B_{i}' if i < 3 else '')\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0].axhline(1, color='black', linewidth=0.5, linestyle='--', alpha=0.3)\n",
    "for b in borders:\n",
    "    axes[0].axvline(b, color='red', linewidth=0.5, alpha=0.3)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('Basis Function Value', fontsize=12)\n",
    "axes[0].set_title('1D Linear Basis Functions', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.1, 1.2)\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Verify partition of unity\n",
    "sum_basis = basis_1d.sum(axis=1)\n",
    "axes[1].plot(x_range, sum_basis, linewidth=2, color='purple')\n",
    "axes[1].axhline(1, color='red', linewidth=2, linestyle='--', label='Unity')\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('Sum of All Basis Functions', fontsize=12)\n",
    "axes[1].set_title('Partition of Unity (should = 1)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.9, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"1D Basis functions:\")\n",
    "print(f\"  Number of basis functions: {len(borders)}\")\n",
    "print(f\"  Partition of unity check: min={sum_basis.min():.6f}, max={sum_basis.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize 2D Spline Basis Functions\n",
    "\n",
    "Bilinear interpolation basis functions (tensor products of 1D bases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_2d_basis_function(x1_vals, x2_vals, borders, i, j):\n",
    "    \"\"\"Compute single 2D basis function B_i(x1) * B_j(x2)\"\"\"\n",
    "    basis_1d_x1 = compute_1d_basis_functions(x1_vals, borders)\n",
    "    basis_1d_x2 = compute_1d_basis_functions(x2_vals, borders)\n",
    "    \n",
    "    # Tensor product\n",
    "    basis_2d = basis_1d_x1[:, i:i+1] * basis_1d_x2[:, j:j+1].T\n",
    "    return basis_2d\n",
    "\n",
    "# Create 2D grid for visualization\n",
    "n_points = 100\n",
    "x1_grid = np.linspace(borders[0], borders[-1], n_points)\n",
    "x2_grid = np.linspace(borders[0], borders[-1], n_points)\n",
    "X1, X2 = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "# Plot a selection of 2D basis functions (3D surface)\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "n_show = min(4, NUM_GRIDS)  # Show 4x4 grid of basis functions\n",
    "indices_to_show = np.linspace(0, NUM_GRIDS, n_show, dtype=int)\n",
    "\n",
    "plot_idx = 1\n",
    "for i in indices_to_show:\n",
    "    for j in indices_to_show:\n",
    "        ax = fig.add_subplot(n_show, n_show, plot_idx, projection='3d')\n",
    "        \n",
    "        basis_2d = compute_2d_basis_function(x1_grid, x2_grid, borders, i, j)\n",
    "        \n",
    "        surf = ax.plot_surface(X1, X2, basis_2d, cmap=cm.viridis, \n",
    "                               linewidth=0, antialiased=True, alpha=0.9)\n",
    "        ax.set_xlabel('x\u2081', fontsize=8)\n",
    "        ax.set_ylabel('x\u2082', fontsize=8)\n",
    "        ax.set_zlabel('B', fontsize=8)\n",
    "        ax.set_title(f'B_{i},{j}(x\u2081,x\u2082)', fontsize=10, fontweight='bold')\n",
    "        ax.view_init(elev=25, azim=45)\n",
    "        ax.set_zlim(0, 1.1)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "plt.suptitle('2D Bilinear Basis Functions (Tensor Products)', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"2D Basis functions:\")\n",
    "print(f\"  Total number: {(NUM_GRIDS+1)**2}\")\n",
    "print(f\"  Each is a tensor product: B_ij(x1,x2) = B_i(x1) * B_j(x2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Show 2D basis functions as heatmaps\n",
    "fig, axes = plt.subplots(n_show, n_show, figsize=(12, 12))\n",
    "\n",
    "for idx_i, i in enumerate(indices_to_show):\n",
    "    for idx_j, j in enumerate(indices_to_show):\n",
    "        ax = axes[idx_i, idx_j]\n",
    "        \n",
    "        basis_2d = compute_2d_basis_function(x1_grid, x2_grid, borders, i, j)\n",
    "        \n",
    "        im = ax.imshow(basis_2d, extent=[borders[0], borders[-1], borders[0], borders[-1]],\n",
    "                      origin='lower', cmap='viridis', vmin=0, vmax=1)\n",
    "        ax.set_xlabel('x\u2081', fontsize=8)\n",
    "        ax.set_ylabel('x\u2082', fontsize=8)\n",
    "        ax.set_title(f'B_{i},{j}', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Add grid lines at borders\n",
    "        for b in borders:\n",
    "            ax.axhline(b, color='white', linewidth=0.5, alpha=0.3)\n",
    "            ax.axvline(b, color='white', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), label='Basis Value', shrink=0.8)\n",
    "plt.suptitle('2D Bilinear Basis Functions (Heatmaps)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Learned Functions\n",
    "\n",
    "Extract and visualize the learned lookup tables from trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned 1D functions from trained 1D KAN\n",
    "learned_1d = model_1d.func_parameter.detach().cpu().numpy()  # [num_grids+1, output_dim, input_dim]\n",
    "\n",
    "# Plot a selection of learned 1D functions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show first 8 input dimensions\n",
    "for in_idx in range(min(8, 32)):\n",
    "    ax = axes[in_idx]\n",
    "    func_values = learned_1d[:, 0, in_idx]  # output_dim=1, so index 0\n",
    "    \n",
    "    ax.plot(borders, func_values, 'o-', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Grid Point', fontsize=10)\n",
    "    ax.set_ylabel('Function Value', fontsize=10)\n",
    "    ax.set_title(f'Learned f(x_{in_idx})', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(0, color='black', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Learned 1D Functions from 1D KAN', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"1D Learned functions shape: {learned_1d.shape}\")\n",
    "print(f\"  Value range: [{learned_1d.min():.3f}, {learned_1d.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned 2D functions from trained 2D KAN\n",
    "if model_2d is not None:\n",
    "    learned_2d = model_2d.kan.func_parameter.detach().cpu().numpy()  # [num_grids+1, num_grids+1, output_dim, input_dim//2]\n",
    "    \n",
    "    # Plot a selection of learned 2D functions\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Show first 8 input pairs (16 dimensions)\n",
    "    for pair_idx in range(min(8, 16)):\n",
    "        ax = axes[pair_idx]\n",
    "        func_2d = learned_2d[:, :, 0, pair_idx]  # output_dim=1, so index 0\n",
    "        \n",
    "        im = ax.imshow(func_2d, cmap='RdBu_r', origin='lower')\n",
    "        ax.set_xlabel('x\u2082 grid', fontsize=10)\n",
    "        ax.set_ylabel('x\u2081 grid', fontsize=10)\n",
    "        ax.set_title(f'Learned f(x_{2*pair_idx}, x_{2*pair_idx+1})', fontsize=11, fontweight='bold')\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.suptitle('Learned 2D Functions from 2D KAN (lmKAN)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"2D Learned functions shape: {learned_2d.shape}\")\n",
    "    print(f\"  Value range: [{learned_2d.min():.3f}, {learned_2d.max():.3f}]\")\n",
    "else:\n",
    "    print(\"2D KAN not trained (CUDA required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Parameter Efficiency**: 2D KAN has more parameters than 1D KAN at the same grid resolution, but achieves better approximation\n",
    "\n",
    "2. **Basis Functions**:\n",
    "   - 1D: Linear interpolation (\\\"hat\\\" functions)\n",
    "   - 2D: Bilinear interpolation (tensor products)\n",
    "\n",
    "3. **Performance**: 2D lookup KANs exploit correlations between paired inputs, leading to better capacity per function\n",
    "\n",
    "4. **CDF Grid**: Non-uniform grid places more resolution near zero, matching typical data distributions\n",
    "\n",
    "5. **Hessian Regularization**: Critical for training stability and smooth learned functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Layer Architecture Explanation\n",
    "\n",
    "### Data Flow Through lmKAN (2D) Layer:\n",
    "\n",
    "```\n",
    "Input: x \u2208 R^32  (batch_size, 32)\n",
    "   \u2193\n",
    "BatchNorm (affine=False)\n",
    "   \u2193\n",
    "Transpose to batch-last: (32, batch_size)\n",
    "   \u2193\n",
    "Pair consecutive dimensions:\n",
    "  (x\u2080, x\u2081), (x\u2082, x\u2083), ..., (x\u2083\u2080, x\u2083\u2081)\n",
    "  \u2192 16 pairs\n",
    "   \u2193\n",
    "Apply Laplace CDF to each coordinate:\n",
    "  x_cdf = 0.5\u00b7exp(-|x|)  if x < 0\n",
    "        = 1 - 0.5\u00b7exp(-|x|)  if x \u2265 0\n",
    "   \u2193\n",
    "Find grid cell for each pair:\n",
    "  (i, j) = grid_cell(x_cdf\u2081, x_cdf\u2082)\n",
    "   \u2193\n",
    "Bilinear interpolation:\n",
    "  f(x\u2081,x\u2082) = (1-\u03b4\u2081)(1-\u03b4\u2082)\u00b7p[i,j]\n",
    "           + (1-\u03b4\u2081)\u00b7\u03b4\u2082\u00b7p[i,j+1]\n",
    "           + \u03b4\u2081\u00b7(1-\u03b4\u2082)\u00b7p[i+1,j]\n",
    "           + \u03b4\u2081\u00b7\u03b4\u2082\u00b7p[i+1,j+1]\n",
    "   \u2193\n",
    "Sum over all 16 pairs \u2192 Output \u2208 R^1\n",
    "```\n",
    "\n",
    "### Parameter Tensor Structure:\n",
    "\n",
    "```python\n",
    "# 2D KAN parameters\n",
    "shape = [num_grids+1, num_grids+1, output_dim, input_dim//2]\n",
    "# Example: [9, 9, 1, 16] for num_grids=8, input=32, output=1\n",
    "# Total: 9\u00d79\u00d71\u00d716 = 1,296 parameters\n",
    "\n",
    "# 1D KAN parameters\n",
    "shape = [num_grids+1, output_dim, input_dim]\n",
    "# Example: [9, 1, 32] for num_grids=8, input=32, output=1\n",
    "# Total: 9\u00d71\u00d732 = 288 parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "results = {\n",
    "    '1D_KAN': {\n",
    "        'params': int(params_1d),\n",
    "        'final_test_mse': float(test_losses_1d[-1]),\n",
    "        'training_time': float(time_1d),\n",
    "    },\n",
    "}\n",
    "\n",
    "if model_2d is not None:\n",
    "    results['2D_KAN'] = {\n",
    "        'params': int(params_2d),\n",
    "        'final_test_mse': float(test_losses_2d[-1]),\n",
    "        'training_time': float(time_2d),\n",
    "    }\n",
    "\n",
    "import json\n",
    "with open('comparison_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to comparison_results.json\")\n",
    "print(\"\\nNotebook execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}